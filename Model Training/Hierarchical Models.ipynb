{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3878dac",
   "metadata": {},
   "source": [
    "# Hierarchical Binary Classifiers\n",
    "The premise of this model is to instead of identifying all 4 classes at once, identify whether a stroke is long axis vs short axis. Then, based on that decision determine whether the short axis stroke is breastroke or butterfly, and whether the long axis stroke is freestyle or backstroke. The reason for this specific hierarchical tree is because the model was facing significant issues differentiating between freestyle and backstroke, which was somewhat alleviated by introducing a DNN rather than a CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9313a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, optimizers\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization,\\\n",
    "                                    Add, AveragePooling2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow import Tensor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.initializers import he_normal\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import sklearn\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce7c6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 32, 12, 3)\n",
      "(2400, 2)\n",
      "(97, 32, 12, 3)\n",
      "(97, 2)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "\n",
    "x_train = np.load('x_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "x_test = np.load('x2_test.npy')\n",
    "y_test = np.load('y2_test.npy')\n",
    "xt = x_test\n",
    "yt = y_test\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "yt = keras.utils.to_categorical(yt, num_classes)\n",
    "tl = np.argmax(yt, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "022b4b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"test.evaluate(x_test, y_test, verbose=1)\\npredicted_labels=np.argmax(test.predict(xt), axis=1)\\ntl = np.argmax(yt, axis=1)\\n#tl = yt\\nconfusion_mat = confusion_matrix(tl, predicted_labels)\\nprint(confusion_mat)\\nprint(f'Model Accuracy based on testing data: {(np.trace(confusion_mat))/np.sum(confusion_mat)}')\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longAxisBC0 = tf.keras.models.load_model('testModel74')\n",
    "longAxisBC1 = tf.keras.models.load_model('testModel75')\n",
    "longAxisBC2 = tf.keras.models.load_model('testModel76')\n",
    "longAxisBC3 = tf.keras.models.load_model('testModel77')\n",
    "#longAxisBC4 = tf.keras.models.load_model('testModel78')\n",
    "\n",
    "shortAxisBC0 = tf.keras.models.load_model('testModel81')\n",
    "shortAxisBC1 = tf.keras.models.load_model('testModel82')\n",
    "shortAxisBC2 = tf.keras.models.load_model('testModel83')\n",
    "shortAxisBC3 = tf.keras.models.load_model('testModel84')\n",
    "shortAxisBC4 = tf.keras.models.load_model('testModel85')\n",
    "\n",
    "axisBC0 = tf.keras.models.load_model('testModel90')\n",
    "axisBC1 = tf.keras.models.load_model('testModel91')\n",
    "axisBC2 = tf.keras.models.load_model('testModel92')\n",
    "axisBC3 = tf.keras.models.load_model('testModel93')\n",
    "axisBC4 = tf.keras.models.load_model('testModel94')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c058e0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 29ms/step - loss: 0.6413 - accuracy: 0.6804\n",
      "[[41 11]\n",
      " [27 18]]\n",
      "Model Accuracy based on testing data: 0.6082474226804123\n"
     ]
    }
   ],
   "source": [
    "shortAxisBC1.evaluate(x_test, y_test, verbose=1)\n",
    "predicted_labels=np.argmax(shortAxisBC0.predict(xt), axis=1)\n",
    "tl = np.argmax(yt, axis=1)\n",
    "#tl = yt\n",
    "confusion_mat = confusion_matrix(tl, predicted_labels)\n",
    "print(confusion_mat)\n",
    "print(f'Model Accuracy based on testing data: {(np.trace(confusion_mat))/np.sum(confusion_mat)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d53be475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hard Voting Ensemble\n",
    "#Step 1: test axis\n",
    "y_check = []\n",
    "count = 0\n",
    "for data in x_test:\n",
    "    x1 = np.argmax(shortAxisBC0.predict(np.reshape(data, (1, 32, 12, 3))), axis=1)\n",
    "    x2 = np.argmax(shortAxisBC1.predict(np.reshape(data, (1, 32, 12, 3))), axis=1)\n",
    "    x3 = np.argmax(shortAxisBC2.predict(np.reshape(data, (1, 32, 12, 3))), axis=1)\n",
    "    x4 = np.argmax(shortAxisBC3.predict(np.reshape(data, (1, 32, 12, 3))), axis=1)\n",
    "    x5 = np.argmax(shortAxisBC4.predict(np.reshape(data, (1, 32, 12, 3))), axis=1)\n",
    "    array = [x1, x2, x3, x4, x5]\n",
    "    c=0\n",
    "    count += 1\n",
    "    for i in array:\n",
    "        if i == [0]:\n",
    "            c+=1\n",
    "        else:\n",
    "            pass\n",
    "    if c >=3:\n",
    "        y_check.append(0)\n",
    "    else:\n",
    "        y_check.append(1)\n",
    "    if count%100 == 0:\n",
    "        print(\"100 count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4596aa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "conf = [[0.3, 0.7], [0.2, 0.8], [0.5, 0.5], [0.7, 0.3], [0.1, 0.9]]\n",
    "\n",
    "num_models = len(conf)  # Number of models\n",
    "\n",
    "# Initialize a dictionary to store accumulated class probabilities\n",
    "conf_array = np.array(conf)\n",
    "\n",
    "# Calculate the average of values at index 0 and index 1\n",
    "conf_array = [np.mean(conf_array[:, 0]), np.mean(conf_array[:, 1])]  # Average of index 0 values\n",
    "print(np.argmax(conf_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65dfa478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 count\n",
      "10 count\n",
      "10 count\n",
      "10 count\n",
      "10 count\n",
      "10 count\n",
      "10 count\n",
      "10 count\n",
      "10 count\n"
     ]
    }
   ],
   "source": [
    "#Step 1: test axis\n",
    "y_check = []\n",
    "count = 0\n",
    "for data in x_test:\n",
    "    #x1 = shortAxisBC0.predict(np.reshape(data, (1, 32, 12, 3)))\n",
    "    x2 = shortAxisBC1.predict(np.reshape(data, (1, 32, 12, 3)))\n",
    "    x3 = shortAxisBC2.predict(np.reshape(data, (1, 32, 12, 3)))\n",
    "    x4 = shortAxisBC3.predict(np.reshape(data, (1, 32, 12, 3)))\n",
    "    x5 = shortAxisBC4.predict(np.reshape(data, (1, 32, 12, 3)))\n",
    "    conf = [x2, x3, x4, x5]\n",
    "    count += 1\n",
    "\n",
    "    num_models = len(conf)  # Number of models\n",
    "\n",
    "    # Initialize a dictionary to store accumulated class probabilities\n",
    "    conf_array = np.array(conf).reshape(-1, 2)\n",
    "\n",
    "    # Calculate the average of values at index 0 and index 1\n",
    "    conf_array = [np.mean(conf_array[:, 0]), np.mean(conf_array[:, 1])]  # Average of index 0 values\n",
    "    y_check.append(np.argmax(conf_array))\n",
    "    if count%10 == 0:\n",
    "        print(\"10 count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42819539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45  7]\n",
      " [11 34]]\n",
      "Model Accuracy based on testing data: 0.8144329896907216\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = np.array(y_check)\n",
    "tl = np.argmax(yt, axis=1)\n",
    "confusion_mat = confusion_matrix(tl, predicted_labels)\n",
    "print(confusion_mat)\n",
    "print(f'Model Accuracy based on testing data: {(np.trace(confusion_mat))/np.sum(confusion_mat)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc37ef",
   "metadata": {},
   "source": [
    "### Long Axis Series (Free vs Back Classifier)\n",
    "All ensemble types showed no signs of improvement\n",
    "\n",
    "### Short Axis Series (Fly vs Breast Classifier)\n",
    "Original\n",
    "approx 72\n",
    "##### Hard Voting Ensemble\n",
    "eh\n",
    "\n",
    "##### Soft Voting Ensemble\n",
    "[[45  7]\n",
    " [11 34]]\n",
    "Model Accuracy based on testing data: 0.8144329896907216\n",
    "\n",
    "### Axis BC Series (FB vs FB)\n",
    "Original\n",
    "[[77 18]\n",
    " [27 70]]\n",
    "Model Accuracy based on testing data: 0.765625\n",
    "##### Hard Voting Ensemble\n",
    "[[84 11]\n",
    " [30 67]]\n",
    "Model Accuracy based on testing data: 0.7864583333333334\n",
    "##### Soft Voting Ensemble\n",
    "[[85 10]\n",
    " [32 65]]\n",
    "Model Accuracy based on testing data: 0.78125"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
